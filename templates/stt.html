{% extends "base.html" %}

{% block title %}Speech to Text - Cloudflare Showcase{% endblock %}

{% block content %}
<script>
// Define a WebSocket debug extension for logging WebSocket events
htmx.defineExtension('ws-debug', {
    onEvent: function(name, evt) {
        const logDiv = document.getElementById('message-log');
        if (!logDiv) return;

        switch(name) {
            case 'htmx:wsBeforeMessage':
                this.logMessage(evt.detail.message, 'RECEIVED');
                break;
            case 'htmx:wsBeforeSend':
                this.logMessage(evt.detail.message, 'SENT');
                break;
            case 'htmx:wsOpen':
                this.logMessage('WebSocket Connected', 'CONNECT');
                break;
            case 'htmx:wsClose':
                this.logMessage('WebSocket Disconnected', 'DISCONNECT');
                break;
            case 'htmx:wsError':
                this.logMessage('WebSocket Error', 'ERROR');
                break;
            case 'htmx:wsConnecting':
                this.logMessage('Connecting to WebSocket...', 'CONNECT');
                break;
        }
    },
    logMessage: function(msg, direction) {
        const ARROWS = {
            RECEIVED: 'üì•',
            SENT: '‚¨ÜÔ∏è',
            CONNECT: 'üîÑ',
            DISCONNECT: 'üî¥',
            ERROR: '‚ö†Ô∏è'
        };

        let msgType = '';
        let displayMsg = msg;
        try {
            const jsonMsg = JSON.parse(msg);
            if (jsonMsg && typeof jsonMsg === 'object') {
                // Remove audio_data before logging
                const { audio_data, ...cleanMsg } = jsonMsg;
                displayMsg = JSON.stringify(cleanMsg);
                msgType = '{json} ';
            }
        } catch (e) {
            msgType = '[text] ';
        }

        AppLogger.logMessage(`${ARROWS[direction]} ${msgType}${displayMsg}`, direction === 'ERROR' ? 'ERROR' : 'INFO');
    }
});

htmx.defineExtension("ws-connection-status", {
    onEvent: function (name, evt) {
        const connectionStatus = document.getElementById("connection-status");
        const startButton = document.getElementById("startRecording");
        if (!connectionStatus || !startButton) return;

        const baseClasses = "inline-block text-sm font-medium rounded-full px-4 py-1 w-fit";
        const buttonBaseClasses = "bg-blue-500 hover:bg-blue-600 text-white font-semibold py-2 px-4 rounded flex items-center";

        switch (name) {
            case "htmx:wsConnecting":
                connectionStatus.textContent = "Connecting...";
                connectionStatus.className = `${baseClasses} bg-yellow-100 text-yellow-600`;
                startButton.disabled = true;
                startButton.className = `${buttonBaseClasses} opacity-50 cursor-not-allowed`;
                break;
            case "htmx:wsOpen":
                connectionStatus.textContent = "Connected";
                connectionStatus.className = `${baseClasses} bg-green-100 text-green-600`;
                startButton.disabled = false;
                startButton.className = buttonBaseClasses;
                break;
            case "htmx:wsClose":
                connectionStatus.textContent = "Disconnected - Reconnecting...";
                connectionStatus.className = `${baseClasses} bg-red-100 text-red-600`;
                startButton.disabled = true;
                startButton.className = `${buttonBaseClasses} opacity-50 cursor-not-allowed`;
                break;
        }
    }
});
</script>

<div class="container mx-auto px-4 py-8"
     hx-ext="ws,ws-debug,ws-connection-status"
     ws-connect="/stt/ws">
    <div class="relative">
        <!-- Connection Status Badge -->
        <aside id="connection-status" 
               class="absolute left-1/2 -translate-x-1/2 -top-3 z-10 inline-block text-sm font-medium rounded-full px-4 py-1 w-fit bg-red-100 text-red-600 shadow-sm">
            Disconnected
        </aside>
        
        <h1 class="text-3xl font-bold mb-6">Speech to Text</h1>
        <div class="bg-white shadow-md rounded-lg p-6">
            <p class="text-gray-600 mb-4">Convert your speech to text using advanced AI technology.</p>
            <div class="space-y-4">
                <div class="flex space-x-4">
                    <button id="startRecording" disabled class="bg-blue-500 hover:bg-blue-600 text-white font-semibold py-2 px-4 rounded flex items-center opacity-50 cursor-not-allowed">
                        <svg class="w-5 h-5 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="6" fill="currentColor"/>
                        </svg>
                        Start Recording
                    </button>
                    <button id="stopRecording" class="bg-red-500 hover:bg-red-600 text-white font-semibold py-2 px-4 rounded hidden flex items-center">
                        <svg class="w-5 h-5 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                            <rect x="8" y="8" width="8" height="8" fill="currentColor"/>
                        </svg>
                        Stop Recording
                    </button>
                </div>
                <div id="recordingTime" class="text-xl font-mono text-gray-600 hidden">00:00</div>
                <div id="audioPreview" class="hidden">
                    <audio id="recordingPlayback" controls class="w-full"></audio>
                </div>
                <div id="status" class="text-gray-600 font-medium"></div>
                <div id="word-count" class="text-sm text-gray-500">Words: 0</div>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                    <div>
                        <label for="transcription" class="block text-sm font-medium text-gray-700 mb-1">Transcription</label>
                        <div id="transcription-container" class="space-y-2">
                            <!-- Transcriptions will be added here dynamically -->
                        </div>
                    </div>
                    <div>
                        <label for="word-timing" class="block text-sm font-medium text-gray-700 mb-1">Word Timing</label>
                        <textarea id="word-timing" class="w-full h-32 p-2 border rounded-lg" placeholder="Word timing will appear here..." readonly></textarea>
                    </div>
                </div>
                <div>
                    <label for="vtt-content" class="block text-sm font-medium text-gray-700 mb-1">VTT Subtitles</label>
                    <textarea id="vtt-content" class="w-full h-32 p-2 border rounded-lg font-mono text-sm" placeholder="VTT content will appear here..." readonly></textarea>
                </div>
            </div>
        </div>
    </div>

    {% include "components/logging.html" %}
</div>

<script>
// First, define the AudioWorklet code as a string
const audioWorkletCode = `
class AudioProcessor extends AudioWorkletProcessor {
    constructor() {
        super();
        this.buffer = new Float32Array();
        this.silenceThreshold = 0.001;        // Lowered threshold for silence detection
        this.silenceCounter = 0;
        this.minSilenceDuration = 40;         // Frames of silence needed for a break (40/128 ‚âà 0.3s)
        this.maxSilenceDuration = 256;        // 2s max silence
        this.minChunkDuration = 16000;        // 1 second minimum (lowered from 2s)
        this.maxChunkDuration = 160000;       // 10 seconds maximum
        this.hasDetectedSpeech = false;
        this.consecutiveSpeechFrames = 0;
        this.minSpeechFrames = 2;             // Lowered from 3
        this.preSpeechBuffer = new Float32Array(4000);
        this.preSpeechWritePos = 0;

        // Handle messages from the main thread
        this.port.onmessage = (event) => {
            if (event.data.type === 'flush' && this.buffer.length > 0) {
                // When flushing, send whatever we have regardless of length
                this.sendBuffer(this.buffer, false, true);
                this.buffer = new Float32Array();
            }
        };
    }

    process(inputs, outputs, parameters) {
        const input = inputs[0];
        if (!input || !input[0]) return true;

        const inputChannel = input[0];
        
        // Always append incoming audio to buffer
        const newBuffer = new Float32Array(this.buffer.length + inputChannel.length);
        newBuffer.set(this.buffer);
        newBuffer.set(inputChannel, this.buffer.length);
        this.buffer = newBuffer;

        // Calculate RMS for silence detection
        const rms = Math.sqrt(inputChannel.reduce((acc, val) => acc + val * val, 0) / inputChannel.length);
        
        if (rms > this.silenceThreshold) {
            this.consecutiveSpeechFrames++;
            this.silenceCounter = 0;
            this.hasDetectedSpeech = true;
        } else {
            this.consecutiveSpeechFrames = 0;
            this.silenceCounter++;
        }

        // Send buffer if it's long enough or if we've detected speech and hit silence
        if (this.buffer.length >= this.maxChunkDuration || 
            (this.hasDetectedSpeech && this.buffer.length >= this.minChunkDuration && this.silenceCounter >= this.minSilenceDuration)) {
            
            this.sendBuffer(this.buffer, this.buffer.length >= this.maxChunkDuration, false);
            this.buffer = new Float32Array();
            this.hasDetectedSpeech = false;
        }

        return true;
    }

    sendBuffer(buffer, isStreaming, isFlush = false) {
        // Only enforce minimum samples if not flushing
        if (!isFlush && buffer.length < 128) return;

        // Convert to 16-bit PCM
        const pcmData = new Int16Array(buffer.length);
        for (let i = 0; i < buffer.length; i++) {
            pcmData[i] = Math.min(32767, Math.max(-32768, Math.round(buffer[i] * 32767)));
        }

        this.port.postMessage({
            type: 'audio',
            buffer: pcmData.buffer,
            isStreaming: isStreaming
        }, [pcmData.buffer]);
    }
}

registerProcessor('audio-processor', AudioProcessor);
`;

const AudioRecorder = {
    audioContext: null,
    audioWorklet: null,
    mediaStreamSource: null,
    stream: null,
    webSocket: null,
    isRecording: false,

    init() {
        AppLogger.logMessage('Initializing Audio Recorder...', 'INFO');
        
        // Check browser compatibility
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            AppLogger.logMessage('Browser does not support audio recording', 'ERROR');
            throw new Error('Audio recording not supported');
        }
        AppLogger.logMessage('Audio recording is supported', 'SUCCESS');

        // Setup click handlers
        const startButton = document.getElementById('startRecording');
        const stopButton = document.getElementById('stopRecording');
        
        startButton?.addEventListener('click', () => this.startRecording());
        stopButton?.addEventListener('click', () => this.stopRecording());

        // Setup WebSocket handlers
        document.body.addEventListener('htmx:wsOpen', (e) => {
            const wsData = e.detail.elt['htmx-internal-data']?.webSocket;
            if (wsData && wsData.socket) {
                this.webSocket = wsData.socket;
                AppLogger.logMessage('WebSocket successfully initialized', 'SUCCESS');
            }
        });

        document.body.addEventListener('htmx:wsClose', () => {
            this.webSocket = null;
            AppLogger.logMessage('Audio WebSocket disconnected', 'ERROR');
        });

        // Handle transcription updates
        document.body.addEventListener('htmx:wsBeforeMessage', (e) => {
            try {
                if (typeof e.detail.message === 'string') {
                    const data = JSON.parse(e.detail.message);
                    
                    // Skip the welcome message as it's handled by the server
                    if (data.text === "Connected! Click Start Recording to begin...") {
                        return;
                    }

                    // Update transcription display
                    const container = document.getElementById('transcription-container');
                    if (container) {
                        // Create a new transcription entry
                        const entry = document.createElement('div');
                        entry.className = 'flex items-center space-x-2 p-2 border rounded';
                        
                        // Add play button if we have audio data
                        if (data.audio_data) {
                            const audioBlob = base64ToBlob(data.audio_data, 'audio/wav');
                            const audioUrl = URL.createObjectURL(audioBlob);
                            
                            // Play button
                            const playButton = document.createElement('button');
                            playButton.className = 'flex-shrink-0 text-blue-500 hover:text-blue-600';
                            playButton.innerHTML = `
                                <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M14.752 11.168l-3.197-2.132A1 1 0 0010 9.87v4.263a1 1 0 001.555.832l3.197-2.132a1 1 0 000-1.664z" />
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                                </svg>
                            `;
                            
                            const audio = new Audio(audioUrl);
                            playButton.addEventListener('click', () => {
                                audio.play();
                            });
                            
                            // Download button
                            const downloadButton = document.createElement('a');
                            downloadButton.className = 'flex-shrink-0 text-green-500 hover:text-green-600';
                            downloadButton.href = audioUrl;
                            downloadButton.download = `audio-${Date.now()}.wav`;
                            downloadButton.innerHTML = `
                                <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16v1a3 3 0 003 3h10a3 3 0 003-3v-1m-4-4l-4 4m0 0l-4-4m4 4V4" />
                                </svg>
                            `;
                            
                            const buttonContainer = document.createElement('div');
                            buttonContainer.className = 'flex space-x-2';
                            buttonContainer.appendChild(playButton);
                            buttonContainer.appendChild(downloadButton);
                            entry.appendChild(buttonContainer);
                        }

                        // Add text
                        const text = document.createElement('span');
                        text.className = 'flex-grow';
                        text.textContent = data.text;
                        entry.appendChild(text);
                        
                        // Add to container
                        container.appendChild(entry);
                        container.scrollTop = container.scrollHeight;
                    }

                    // Update word timing display
                    const wordTimingArea = document.getElementById('word-timing');
                    if (wordTimingArea && data.words) {
                        const wordTiming = data.words.map(w => 
                            `${w.word.trim()}: ${w.start.toFixed(2)}s - ${w.end.toFixed(2)}s`
                        ).join('\n');
                        wordTimingArea.value = wordTiming;
                        wordTimingArea.scrollTop = wordTimingArea.scrollHeight;
                    }

                    // Update VTT display
                    const vttArea = document.getElementById('vtt-content');
                    if (vttArea && data.vtt) {
                        vttArea.value = data.vtt;
                        vttArea.scrollTop = vttArea.scrollHeight;
                    }

                    // Update word count
                    if (data.word_count) {
                        const wordCountElement = document.getElementById('word-count');
                        if (wordCountElement) {
                            wordCountElement.textContent = `Words: ${data.word_count}`;
                        }
                    }

                    // Log metrics if available
                    if (data.avg_logprob !== null) {
                        AppLogger.logMessage(`Confidence Metrics:
                            avg_logprob: ${data.avg_logprob?.toFixed(4)}
                            no_speech_prob: ${data.no_speech_prob?.toFixed(4)}
                            temperature: ${data.temperature?.toFixed(4)}
                            compression_ratio: ${data.compression_ratio?.toFixed(4)}`, 'INFO');
                    }
                }
            } catch (error) {
                AppLogger.logMessage(`Error processing message: ${error}`, 'ERROR');
            }
        });
    },

    async sendAudioChunk(audioData, isStreaming = false) {
        if (this.webSocket && this.webSocket.readyState === WebSocket.OPEN) {
            try {
                // Send the audio data
                this.webSocket.send(audioData);
                const duration = (audioData.byteLength / 2) / 16000; // 16-bit samples at 16kHz
                AppLogger.logMessage(`üîç Sent audio chunk (${duration.toFixed(2)}s)`, 'INFO');
            } catch (error) {
                AppLogger.logMessage(`Failed to send audio chunk: ${error}`, 'ERROR');
            }
        } else {
            AppLogger.logMessage('WebSocket not ready, cannot send audio', 'ERROR');
        }
    },

    async startRecording() {
        try {
            if (!this.webSocket || this.webSocket.readyState !== WebSocket.OPEN) {
                AppLogger.logMessage('WebSocket not ready, cannot start recording', 'ERROR');
                return;
            }

            AppLogger.logMessage('Requesting microphone access...', 'INFO');
            this.stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true,
                    sampleRate: 16000
                }
            });
            AppLogger.logMessage('Microphone access granted', 'SUCCESS');

            // Create AudioContext and initialize AudioWorklet
            this.audioContext = new AudioContext({
                sampleRate: 16000,
                channelCount: 1,
                latencyHint: 'interactive'
            });

            // Create a Blob URL for the AudioWorklet code
            const blob = new Blob([audioWorkletCode], { type: 'application/javascript' });
            const workletUrl = URL.createObjectURL(blob);

            // Load and add the AudioWorklet module
            await this.audioContext.audioWorklet.addModule(workletUrl);
            URL.revokeObjectURL(workletUrl);

            // Create the audio processing chain
            this.mediaStreamSource = this.audioContext.createMediaStreamSource(this.stream);
            this.audioWorklet = new AudioWorkletNode(this.audioContext, 'audio-processor');

            // Handle audio data from the worklet
            this.audioWorklet.port.onmessage = async (event) => {
                if (event.data.type === 'audio') {
                    await this.sendAudioChunk(event.data.buffer, event.data.isStreaming);
                }
            };

            // Connect the audio nodes
            this.mediaStreamSource.connect(this.audioWorklet);
            this.audioWorklet.connect(this.audioContext.destination);

            // Send start signal
            this.webSocket.send(JSON.stringify({
                chunk_type: "start",
                timestamp: Date.now()
            }));

            this.isRecording = true;
            AppLogger.logMessage('Started recording', 'INFO');

            // Update UI
            document.getElementById('startRecording')?.classList.add('hidden');
            document.getElementById('stopRecording')?.classList.remove('hidden');

        } catch (error) {
            AppLogger.logMessage(`Failed to start recording: ${error}`, 'ERROR');
        }
    },

    async stopRecording() {
        try {
            AppLogger.logMessage('Stopping recording...', 'INFO');

            // Send any remaining audio data in the worklet
            if (this.audioWorklet) {
                this.audioWorklet.port.postMessage({ type: 'flush' });
                // Longer delay to ensure final audio chunk is processed
                await new Promise(resolve => setTimeout(resolve, 500));
            }

            // Stop the media stream
            if (this.stream) {
                this.stream.getTracks().forEach(track => track.stop());
                this.stream = null;
            }

            // Clean up audio context
            if (this.audioWorklet) {
                this.audioWorklet.disconnect();
                this.audioWorklet = null;
            }
            if (this.mediaStreamSource) {
                this.mediaStreamSource.disconnect();
                this.mediaStreamSource = null;
            }
            if (this.audioContext) {
                await this.audioContext.close();
                this.audioContext = null;
            }

            // Send end signal
            if (this.webSocket && this.webSocket.readyState === WebSocket.OPEN) {
                this.webSocket.send(JSON.stringify({
                    chunk_type: "end",
                    timestamp: Date.now()
                }));
                AppLogger.logMessage('Sent end signal to server', 'INFO');
            }

            this.isRecording = false;

            // Update UI
            document.getElementById('startRecording')?.classList.remove('hidden');
            document.getElementById('stopRecording')?.classList.add('hidden');

        } catch (error) {
            AppLogger.logMessage(`Error stopping recording: ${error}`, 'ERROR');
        }
    }
};

// Initialize the recorder when the page loads
document.addEventListener('DOMContentLoaded', () => {
    AudioRecorder.init();
});

function base64ToBlob(base64, type = 'audio/wav') {
    const binStr = atob(base64);
    const len = binStr.length;
    const arr = new Uint8Array(len);
    for (let i = 0; i < len; i++) {
        arr[i] = binStr.charCodeAt(i);
    }
    return new Blob([arr], { type });
}
</script>
{% endblock %} 