{% extends "base.html" %}

{% block title %}Speech to Text - Cloudflare Showcase{% endblock %}

{% block content %}
<script>
// Define a WebSocket debug extension for logging WebSocket events
htmx.defineExtension('ws-debug', {
    onEvent: function(name, evt) {
        const logDiv = document.getElementById('message-log');
        if (!logDiv) return;

        switch(name) {
            case 'htmx:wsBeforeMessage':
                this.logMessage(evt.detail.message, 'RECEIVED');
                break;
            case 'htmx:wsBeforeSend':
                this.logMessage(evt.detail.message, 'SENT');
                break;
            case 'htmx:wsOpen':
                this.logMessage('WebSocket Connected', 'CONNECT');
                break;
            case 'htmx:wsClose':
                this.logMessage('WebSocket Disconnected', 'DISCONNECT');
                break;
            case 'htmx:wsError':
                this.logMessage('WebSocket Error', 'ERROR');
                break;
            case 'htmx:wsConnecting':
                this.logMessage('Connecting to WebSocket...', 'CONNECT');
                break;
        }
    },
    logMessage: function(msg, direction) {
        const ARROWS = {
            RECEIVED: 'üì•',
            SENT: '‚¨ÜÔ∏è',
            CONNECT: 'üîÑ',
            DISCONNECT: 'üî¥',
            ERROR: '‚ö†Ô∏è'
        };

        let msgType = '';
        try {
            JSON.parse(msg);
            msgType = '{json} ';
        } catch (e) {
            msgType = '[text] ';
        }

        AppLogger.logMessage(`${ARROWS[direction]} ${msgType}${msg}`, direction === 'ERROR' ? 'ERROR' : 'INFO');
    }
});

htmx.defineExtension("ws-connection-status", {
    onEvent: function (name, evt) {
        const connectionStatus = document.getElementById("connection-status");
        const startButton = document.getElementById("startRecording");
        if (!connectionStatus || !startButton) return;

        const baseClasses = "inline-block text-sm font-medium rounded-full px-4 py-1 w-fit";
        const buttonBaseClasses = "bg-blue-500 hover:bg-blue-600 text-white font-semibold py-2 px-4 rounded flex items-center";

        switch (name) {
            case "htmx:wsConnecting":
                connectionStatus.textContent = "Connecting...";
                connectionStatus.className = `${baseClasses} bg-yellow-100 text-yellow-600`;
                startButton.disabled = true;
                startButton.className = `${buttonBaseClasses} opacity-50 cursor-not-allowed`;
                break;
            case "htmx:wsOpen":
                connectionStatus.textContent = "Connected";
                connectionStatus.className = `${baseClasses} bg-green-100 text-green-600`;
                startButton.disabled = false;
                startButton.className = buttonBaseClasses;
                break;
            case "htmx:wsClose":
                connectionStatus.textContent = "Disconnected - Reconnecting...";
                connectionStatus.className = `${baseClasses} bg-red-100 text-red-600`;
                startButton.disabled = true;
                startButton.className = `${buttonBaseClasses} opacity-50 cursor-not-allowed`;
                break;
        }
    }
});
</script>

<div class="container mx-auto px-4 py-8"
     hx-ext="ws,ws-debug,ws-connection-status"
     ws-connect="/stt/ws">
    <div class="relative">
        <!-- Connection Status Badge -->
        <aside id="connection-status" 
               class="absolute left-1/2 -translate-x-1/2 -top-3 z-10 inline-block text-sm font-medium rounded-full px-4 py-1 w-fit bg-red-100 text-red-600 shadow-sm">
            Disconnected
        </aside>
        
        <h1 class="text-3xl font-bold mb-6">Speech to Text</h1>
        <div class="bg-white shadow-md rounded-lg p-6">
            <p class="text-gray-600 mb-4">Convert your speech to text using advanced AI technology.</p>
            <div class="space-y-4">
                <div class="flex space-x-4">
                    <button id="startRecording" disabled class="bg-blue-500 hover:bg-blue-600 text-white font-semibold py-2 px-4 rounded flex items-center opacity-50 cursor-not-allowed">
                        <svg class="w-5 h-5 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="6" fill="currentColor"/>
                        </svg>
                        Start Recording
                    </button>
                    <button id="stopRecording" class="bg-red-500 hover:bg-red-600 text-white font-semibold py-2 px-4 rounded hidden flex items-center">
                        <svg class="w-5 h-5 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                            <rect x="8" y="8" width="8" height="8" fill="currentColor"/>
                        </svg>
                        Stop Recording
                    </button>
                </div>
                <div id="recordingTime" class="text-xl font-mono text-gray-600 hidden">00:00</div>
                <div id="audioPreview" class="hidden">
                    <audio id="recordingPlayback" controls class="w-full"></audio>
                </div>
                <div id="status" class="text-gray-600 font-medium"></div>
                <div id="word-count" class="text-sm text-gray-500">Words: 0</div>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                    <div>
                        <label for="transcription" class="block text-sm font-medium text-gray-700 mb-1">Transcription</label>
                        <div id="transcription-container" class="space-y-2">
                            <!-- Transcriptions will be added here dynamically -->
                        </div>
                    </div>
                    <div>
                        <label for="word-timing" class="block text-sm font-medium text-gray-700 mb-1">Word Timing</label>
                        <textarea id="word-timing" class="w-full h-32 p-2 border rounded-lg" placeholder="Word timing will appear here..." readonly></textarea>
                    </div>
                </div>
                <div>
                    <label for="vtt-content" class="block text-sm font-medium text-gray-700 mb-1">VTT Subtitles</label>
                    <textarea id="vtt-content" class="w-full h-32 p-2 border rounded-lg font-mono text-sm" placeholder="VTT content will appear here..." readonly></textarea>
                </div>
            </div>
        </div>
    </div>

    {% include "components/logging.html" %}
</div>

<script>
// First, define the AudioWorklet code as a string
const audioWorkletCode = `
class AudioProcessor extends AudioWorkletProcessor {
    constructor() {
        super();
        this.buffer = new Float32Array();
        this.overlapBuffer = new Float32Array(); // Store previous chunk end for overlap
        this.overlapSize = 8000; // 0.5 seconds overlap at 16kHz
        this.silenceThreshold = 0.003;
        this.silenceCounter = 0;
        this.minSilenceDuration = 25;
        this.minChunkDuration = 16000; // 1 second at 16kHz
        this.maxChunkDuration = 160000; // 10 seconds at 16kHz
        this.lastSendTime = 0;
        this.hasDetectedSpeech = false;
        this.consecutiveSpeechFrames = 0;
        this.minSpeechFrames = 2;
    }

    process(inputs, outputs, parameters) {
        const input = inputs[0];
        if (!input || !input[0]) return true;

        const inputChannel = input[0];
        
        // Calculate RMS for silence detection
        const rms = Math.sqrt(inputChannel.reduce((acc, val) => acc + val * val, 0) / inputChannel.length);
        const isCurrentlySilent = rms < this.silenceThreshold;

        // Track speech detection
        if (!isCurrentlySilent) {
            this.consecutiveSpeechFrames++;
            if (this.consecutiveSpeechFrames >= this.minSpeechFrames) {
                this.hasDetectedSpeech = true;
            }
        } else {
            this.consecutiveSpeechFrames = 0;
        }

        // Update silence counter
        if (isCurrentlySilent) {
            this.silenceCounter++;
        } else {
            this.silenceCounter = 0;
        }

        // Only append audio if we've detected speech or are in an active speech segment
        if (this.hasDetectedSpeech || !isCurrentlySilent) {
            // Append new audio data to buffer
            const newBuffer = new Float32Array(this.buffer.length + inputChannel.length);
            newBuffer.set(this.buffer);
            newBuffer.set(inputChannel, this.buffer.length);
            this.buffer = newBuffer;
        }

        // Determine if we should send the buffer
        let shouldSend = false;

        // Only consider sending if we have detected speech
        if (this.hasDetectedSpeech && this.buffer.length >= this.minChunkDuration) {
            // 1. Natural pause detected
            if (this.silenceCounter >= this.minSilenceDuration) {
                shouldSend = true;
            }
            // 2. Maximum duration reached
            else if (this.buffer.length >= this.maxChunkDuration) {
                shouldSend = true;
            }
            // 3. Time threshold exceeded
            else if ((currentFrame - this.lastSendTime) > 48000) { // ~3 seconds at 16kHz
                shouldSend = true;
            }
        }

        if (shouldSend && this.buffer.length > 0) {
            // Calculate average RMS of the entire buffer to ensure it's not just silence
            const bufferRms = Math.sqrt(
                this.buffer.reduce((acc, val) => acc + val * val, 0) / this.buffer.length
            );

            // Only send if the buffer contains significant audio
            if (bufferRms > this.silenceThreshold * 0.5) {
                // Create buffer with overlap from previous chunk
                const fullBuffer = new Float32Array(this.overlapBuffer.length + this.buffer.length);
                fullBuffer.set(this.overlapBuffer);
                fullBuffer.set(this.buffer, this.overlapBuffer.length);

                // Convert to 16-bit PCM
                const pcmData = new Int16Array(fullBuffer.length);
                for (let i = 0; i < fullBuffer.length; i++) {
                    pcmData[i] = Math.min(32767, Math.max(-32768, Math.round(fullBuffer[i] * 32767)));
                }

                // Store overlap for next chunk
                const overlapStart = Math.max(0, this.buffer.length - this.overlapSize);
                this.overlapBuffer = this.buffer.slice(overlapStart);

                // Send the buffer
                this.port.postMessage({
                    type: 'audio',
                    buffer: pcmData.buffer
                }, [pcmData.buffer]);
            }

            // Reset state
            this.buffer = new Float32Array();
            this.lastSendTime = currentFrame;
            this.silenceCounter = 0;
            this.hasDetectedSpeech = false;
            this.consecutiveSpeechFrames = 0;
        }

        return true;
    }
}

registerProcessor('audio-processor', AudioProcessor);
`;

const AudioRecorder = {
    audioContext: null,
    audioWorklet: null,
    mediaStreamSource: null,
    stream: null,
    webSocket: null,
    isRecording: false,

    init() {
        AppLogger.logMessage('Initializing Audio Recorder...', 'INFO');
        
        // Check browser compatibility
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            AppLogger.logMessage('Browser does not support audio recording', 'ERROR');
            throw new Error('Audio recording not supported');
        }
        AppLogger.logMessage('Audio recording is supported', 'SUCCESS');

        // Setup click handlers
        const startButton = document.getElementById('startRecording');
        const stopButton = document.getElementById('stopRecording');
        
        startButton?.addEventListener('click', () => this.startRecording());
        stopButton?.addEventListener('click', () => this.stopRecording());

        // Setup WebSocket handlers
        document.body.addEventListener('htmx:wsOpen', (e) => {
            const wsData = e.detail.elt['htmx-internal-data']?.webSocket;
            if (wsData && wsData.socket) {
                this.webSocket = wsData.socket;
                AppLogger.logMessage('WebSocket successfully initialized', 'SUCCESS');
            }
        });

        document.body.addEventListener('htmx:wsClose', () => {
            this.webSocket = null;
            AppLogger.logMessage('Audio WebSocket disconnected', 'ERROR');
        });

        // Handle transcription updates
        document.body.addEventListener('htmx:wsBeforeMessage', (e) => {
            try {
                if (typeof e.detail.message === 'string') {
                    const data = JSON.parse(e.detail.message);
                    if (data.text) {
                        // Log only essential fields, explicitly excluding audio_data
                        AppLogger.logMessage(`{json} ${JSON.stringify({
                            text: data.text,
                            is_final: data.is_final,
                            word_count: data.word_count,
                            words: data.words?.map(w => ({ word: w.word, start: w.start, end: w.end })),
                            vtt: data.vtt
                        })}`, 'INFO');

                        const container = document.getElementById('transcription-container');
                        
                        // Create a new transcription entry
                        const entry = document.createElement('div');
                        entry.className = 'flex items-center space-x-2 p-2 border rounded';
                        
                        // Add play button if we have audio data
                        if (data.audio_data) {
                            const audioBlob = this.base64ToBlob(data.audio_data, 'audio/wav');
                            const audioUrl = URL.createObjectURL(audioBlob);
                            
                            // Play button
                            const playButton = document.createElement('button');
                            playButton.className = 'flex-shrink-0 text-blue-500 hover:text-blue-600';
                            playButton.innerHTML = `
                                <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M14.752 11.168l-3.197-2.132A1 1 0 0010 9.87v4.263a1 1 0 001.555.832l3.197-2.132a1 1 0 000-1.664z" />
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                                </svg>
                            `;
                            
                            const audio = new Audio(audioUrl);
                            playButton.addEventListener('click', () => {
                                audio.play();
                            });
                            
                            // Download button
                            const downloadButton = document.createElement('a');
                            downloadButton.className = 'flex-shrink-0 text-green-500 hover:text-green-600';
                            downloadButton.href = audioUrl;
                            downloadButton.download = `audio-${Date.now()}.wav`;
                            downloadButton.innerHTML = `
                                <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16v1a3 3 0 003 3h10a3 3 0 003-3v-1m-4-4l-4 4m0 0l-4-4m4 4V4" />
                                </svg>
                            `;
                            
                            const buttonContainer = document.createElement('div');
                            buttonContainer.className = 'flex space-x-2';
                            buttonContainer.appendChild(playButton);
                            buttonContainer.appendChild(downloadButton);
                            entry.appendChild(buttonContainer);
                        }
                        
                        // Add text
                        const text = document.createElement('span');
                        text.className = 'flex-grow';
                        text.textContent = data.text;
                        entry.appendChild(text);
                        
                        // Add to container
                        container.appendChild(entry);
                        container.scrollTop = container.scrollHeight;
                        
                        if (data.word_count) {
                            document.getElementById('word-count').textContent = `Words: ${data.word_count}`;
                        }
                    }
                }
            } catch (error) {
                console.error('Error processing WebSocket message:', error);
            }
        });
    },

    async startRecording() {
        try {
            if (!this.webSocket || this.webSocket.readyState !== WebSocket.OPEN) {
                AppLogger.logMessage('WebSocket not ready, cannot start recording', 'ERROR');
                return;
            }

            AppLogger.logMessage('Requesting microphone access...', 'INFO');
            this.stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    channelCount: 1,
                    sampleRate: 16000
                }
            });
            AppLogger.logMessage('Microphone access granted', 'SUCCESS');

            this.audioContext = new AudioContext({ sampleRate: 16000 });
            
            // Create a Blob URL for the AudioWorklet code
            const blob = new Blob([audioWorkletCode], { type: 'application/javascript' });
            const workletUrl = URL.createObjectURL(blob);
            
            // Load and initialize the audio worklet
            await this.audioContext.audioWorklet.addModule(workletUrl);
            URL.revokeObjectURL(workletUrl); // Clean up the URL
            
            this.mediaStreamSource = this.audioContext.createMediaStreamSource(this.stream);
            this.audioWorklet = new AudioWorkletNode(this.audioContext, 'audio-processor');
            
            // Handle audio data from the worklet
            this.audioWorklet.port.onmessage = (event) => {
                if (event.data.type === 'audio' && this.webSocket?.readyState === WebSocket.OPEN) {
                    const durationSeconds = event.data.buffer.byteLength / (16000 * 2); // 16kHz, 16-bit audio
                    this.webSocket.send(event.data.buffer);
                    AppLogger.logMessage(`Sent audio chunk (${durationSeconds.toFixed(2)}s)`, 'DEBUG');
                }
            };
            
            // Connect nodes
            this.mediaStreamSource.connect(this.audioWorklet);
            this.audioWorklet.connect(this.audioContext.destination);
            
            this.isRecording = true;

            // Send start signal
            this.webSocket.send(JSON.stringify({
                chunk_type: 'start',
                timestamp: Date.now()
            }));
            AppLogger.logMessage('Started recording', 'INFO');

            this.startRecordingUI();
        } catch (error) {
            AppLogger.logMessage(`Failed to start recording: ${error.message}`, 'ERROR');
            throw error;
        }
    },

    stopRecording() {
        if (this.isRecording) {
            AppLogger.logMessage('Stopping recording...', 'INFO');
            
            this.isRecording = false;
            
            // Cleanup audio context
            if (this.audioWorklet) {
                this.audioWorklet.disconnect();
                this.audioWorklet = null;
            }
            if (this.mediaStreamSource) {
                this.mediaStreamSource.disconnect();
                this.mediaStreamSource = null;
            }
            if (this.audioContext) {
                this.audioContext.close();
                this.audioContext = null;
            }
            
            // Stop all tracks
            if (this.stream) {
                this.stream.getTracks().forEach(track => track.stop());
                this.stream = null;
            }
            
            // Send end signal
            if (this.webSocket?.readyState === WebSocket.OPEN) {
                this.webSocket.send(JSON.stringify({
                    chunk_type: 'end',
                    timestamp: Date.now()
                }));
                AppLogger.logMessage('Sent end signal to server', 'INFO');
            }
            
            this.stopRecordingUI();
        }
    },

    startRecordingUI() {
        document.getElementById('startRecording').classList.add('hidden');
        document.getElementById('stopRecording').classList.remove('hidden');
        document.getElementById('status').textContent = 'Recording...';
    },

    stopRecordingUI() {
        document.getElementById('stopRecording').classList.add('hidden');
        document.getElementById('startRecording').classList.remove('hidden');
        document.getElementById('status').textContent = 'Ready to record';
    },

    base64ToBlob(base64, type = 'audio/wav') {
        const binStr = atob(base64);
        const len = binStr.length;
        const arr = new Uint8Array(len);
        for (let i = 0; i < len; i++) {
            arr[i] = binStr.charCodeAt(i);
        }
        return new Blob([arr], { type });
    }
};

document.addEventListener('DOMContentLoaded', () => {
    try {
        AudioRecorder.init();
    } catch (error) {
        AppLogger.logMessage(`Initialization failed: ${error.message}`, 'ERROR');
        document.getElementById('status').textContent = 'Error: Recording not supported';
    }
});
</script>
{% endblock %} 